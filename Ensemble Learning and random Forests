Ensemble Learning and random Forests

A group of predictors is called an Ensemble; 


Voting Classifiers
-------------------- 

suppose you have trained a few classifiers, each one achieving about 80% accuracy. you may have a logistic Regression classifier, an SVM classifier, A Random Forest classifier, a K-Nearest Neighbors classifier, and perharps a few more

A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most upvtes.

This majority-vote classifier is called a hard voting classifier.

Ensemble methods work best when the predictors are independent from one another as possible. 
One way to get diverse classifiers is to train them using different algorithms.

soft voting often achieves higher performance than hard voting because it gives more weight to higly confident votes.

Bagging and Pasting
----------------------

Another approach is to use the same trainig algorithm for every predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging (short for bootstrap aggregating). When sampling is performed without replacement, its called pasting.

Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of the predictors.

the aggregation function is typically the statistical mode for classification, or the average for regression.

Bagging and Pasting in Scikit-Learn
---------------------------

Out-of-Bag Evaluation
--------------------------

In scikit learn, you can set oob_score=True when creating a baggingClassifier to request an automatic oob evaluation after training.


Random Patches and Random Subspaces
---------------------------

the BaggingClassifier class supports sampling the features as well. This is controlled by two hyperparamters: max_features and bootstrap_features. 

They work the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Tghus, each predictor will be trained on a random subset of the input features

This is partiularly useful when you are dealing with high dimensional inputs (such as images). Sampling both training instances and features is called Random Patches method.


keeping all training instances but sampling features is callled the Random Subspaces method.


Random Forests
----------------------------

Random forests is an ensemble of Decision Trees, generally trained via the bagging method, typically with max_samples set to the size of the training set. 

Instead of building a baggingClassifier and passing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimezed for Decision trees.


The Random forest algorithm introduces extra randomness when growing trees: Instead of searching for the very best feature when splitting a node, it searches for the best feature among subset of features.

This results in a greatre tree diuversity, which (once again) trades a higher bias for a lowe variance, generally yielding an overall better model.



Extra Trees
--------------------------

When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting.

Its possible to make trees even more random by also using random thresholds for each features rather than searching for the best possible thresholds.

A forest of such extremely random trees is simply called an Extremely Randomized Trees ensemble.


Feature Importance
--------------------------

Yet another great quality of random Forests is that they make it easy to measure the relative importtance of each feature. 

Scikit learn measures feature importance by looking at how much the tree nodes that use that feature reduce impurity on average. 

More precisely, it is a weighted average, where each node's weight is equal to the number of training samples that are associated with it.


Boosting
--------------

Boosting refers to any ensemble method that can combine several weak learners into a strong learner.

The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor.

There are many boosting methods available, but by far the most popular are AdaBoost ( Adaptive boosting) and gradient boosting


Stacking
-----------------

stacked generalization is based on a simple idea: Instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don't we train a model to perform this aggregation?


